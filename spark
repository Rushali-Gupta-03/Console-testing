from warpdrive import WarpDrive


wd = WarpDrive()

# your code here

!pip install -r requirements.txt
from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline

spark = SparkSession.builder \
    .master("spark://3.111.255.185:7077") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://3.111.255.185:9000") \
    .appName("Nimbus Remote Worker POC") \
    .getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
hdfs_csv_path = "data_classification.csv"
# Load your dataset
data = spark.read.csv(f'hdfs://3.111.255.185:9000/{hdfs_csv_path}', header=True, inferSchema=True)
data.show()

# Selecting feature columns and target column
feature_columns = ['age', 'race', 'dpros', 'dcaps', 'psa', 'vol', 'gleason']
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

# Preparing the logistic regression model
lr = LogisticRegression(featuresCol="features", labelCol="capsule")

# Create a pipeline to assemble features and fit the model
pipeline = Pipeline(stages=[assembler, lr])

# Split data into training and test sets
train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

# Train the model
model = pipeline.fit(train_data)

# Make predictions on the test data
predictions = model.transform(test_data)

# Show results
predictions.select("capsule", "prediction", "probability").show()

import pandas as pd
# df = pd.DataFrame(predictions.toPandas().to_dict())
df = predictions.toPandas()
wd.save_table(df, name="Predictions on Test Data")
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix, roc_curve, auc
import pandas as pd
# Convert predictions to Pandas dataframe
predictions_pd = predictions.select("capsule", "prediction", "probability").toPandas()

# Extract probability of class 1 for ROC Curve
predictions_pd['probability_1'] = predictions_pd['probability'].apply(lambda x: x[1])

# 1. Confusion Matrix
conf_matrix = confusion_matrix(predictions_pd['capsule'], predictions_pd['prediction'])
conf_matrix_fig = go.Figure(data=go.Heatmap(
    z=conf_matrix,
    x=["Predicted: No", "Predicted: Yes"],
    y=["Actual: No", "Actual: Yes"],
    colorscale="Blues",
    showscale=True
))
conf_matrix_fig.update_layout(
    xaxis_title="Predicted Label",
    yaxis_title="True Label"
)
wd.save_graph(conf_matrix_fig, "Confusion Matrix")

# 2. ROC Curve
fpr, tpr, thresholds = roc_curve(predictions_pd['capsule'], predictions_pd['probability_1'])
roc_auc = auc(fpr, tpr)

roc_curve_fig = go.Figure()
roc_curve_fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f"ROC Curve (AUC = {roc_auc:.2f})", line=dict(color='blue')))
roc_curve_fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='red', dash='dash')))
roc_curve_fig.update_layout(
    xaxis_title="False Positive Rate",
    yaxis_title="True Positive Rate"
)
wd.save_graph(roc_curve_fig, "ROC Curve")

# 3. Probability Distributions
class_0 = predictions_pd[predictions_pd['capsule'] == 0]['probability_1']
class_1 = predictions_pd[predictions_pd['capsule'] == 1]['probability_1']

probability_dist_fig = go.Figure()
probability_dist_fig.add_trace(go.Histogram(x=class_0, name="Class 0", opacity=0.6, marker_color="red"))
probability_dist_fig.add_trace(go.Histogram(x=class_1, name="Class 1", opacity=0.6, marker_color="blue"))

probability_dist_fig.update_layout(
    barmode='overlay',
    xaxis_title="Predicted Probability of Class 1",
    yaxis_title="Frequency"
)
probability_dist_fig.update_traces(opacity=0.75)
probability_dist_fig.show()

wd.save_graph(probability_dist_fig,'Probability Distributions for Each Class')

# 4. Feature Importance (optional)
# coefficients = model.coefficients.toArray()
# features_df = pd.DataFrame({'Feature': feature_columns, 'Coefficient': coefficients})

# feature_importance_fig = px.bar(features_df, x='Coefficient', y='Feature', orientation='h', title="Feature Coefficients in Logistic Regression")
# feature_importance_fig.update_layout(xaxis_title="Coefficient Value", yaxis_title="Feature")
# feature_importance_fig.show()
import psutil
psutil.virtual_memory().free / (1024 ** 3)
spark.stop()
